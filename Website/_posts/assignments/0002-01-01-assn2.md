---
layout: assignment
categories:
 - assignment
title: "Assignment #2: Optimization, Linear & Logistic Regression (due 02/17)"
due:
tags:
 - week-3
permalink: /assn2/
---

## Question 1 [15 Pts]:

Suppose data $$\{x_i\}_{i = 1}^{i = n}$$ is drawn from a normal distribution of _known_ standard deviation $$\sigma$$ and _unknown_ mean $$\mu$$
- Derive the MLE estimate $$\mu_{\text{MLE}}$$
- Derive the MAP estimate $$\mu_{\text{MAP}}$$ assuming the prior is also a normal distribution with mean $$\tau$$ and standard deviation $$\omega$$. What happens as $$n \rightarrow \infty$$?


## Question 2 [30 Pts]:

Consider a Bayesian linear regression model where the outputs $$y$$ are distributed according to a Gaussian likelihood $$p(y\lvert x, \alpha, \beta, \gamma)$$ corresponding to a linear model $$y = \alpha x + \beta + \epsilon$$. Here $$\gamma$$ represents the noise level in the observed data, i.e. it corresponds to the precision of the data likelihood. In this case, the likelihood can be further expressed as:

$$p(y\lvert x, \alpha, \beta, \gamma) = (\sqrt{\frac{\gamma}{2\pi}})^n\exp(-\frac{\gamma(\sum_{i=1}^n\lvert y_i - x_i\alpha - \beta\lvert^2_2)}{2})$$

In a Bayesian setting we would like to assume prior distributions on the unknown parameters $$\alpha$$, $$\beta$$ and $$\gamma$$. Here we assume $$p(\alpha) = \mathcal{N}(\mu_{\alpha}, \sigma_{\alpha})$$, $$p(\beta) = \mathcal{N}(\mu_{\beta}, \sigma_{\beta})$$ and $$p(\gamma) = \textrm{Gam}(\tau, \omega)$$ is a Gamma distribution.

Please write down the posterior conditional distribution for each of those parameters, i.e.,

$$p(\alpha\lvert x, y, \beta, \gamma)$$

$$p(\beta\lvert x, y, \alpha, \gamma)$$

$$p(\gamma\lvert x, y, \alpha, \beta)$$

Comment on your results.

[**HINT**: Might be easier to work with the $$\log$$ of the posterior, and "complete the square" to identify the distribution]

## Question 3 [15 Pts]:

Consider the Bayesian linear regression model with
$$p(y\mid x,w) = \mathcal{N}(y\mid w^{T}\phi(x),\alpha^{-1}I), \ p(w)=\mathcal{N}(w\mid 0,\beta^{-1}I),$$

with $$\alpha = 0.5, \beta  = 0.1$$. Generate a set of $$N = 400$$ noisy observations by uniformly sampling

$$y(x) = \sin(\pi x) + \sin(2 \pi x) + \sin(5 \pi x)\quad x\in[-1,1]$$

Once you've created the observations, perturb the data with a normal distribution with standard deviation set to 10% of the data. Compute the MLE and MAP estimates for the weights $$w$$ using different types and numbers of features $$\phi(x)$$:
- Monomial basis: $$\phi(x) = \{1, x, x^2, x^3, \dots, x^M\}$$
- Fourier basis: $$\phi(x) = \{0, 1, \sin(\pi x), \cos(\pi x), \sin(2\pi x), \cos(2\pi x), \dots, \sin(M\pi x), \cos(M\pi x)\}$$ (this case has a total of $$2M$$ features)
- [Legendre](https://en.wikipedia.org/wiki/Legendre_polynomials) basis: $$\{P_0(x), P_1(x), P_2(x), P_3(x), \dots, P_M(x)\}$$, where $$P_0(x) = 1, P_1(x) = x$$, and subsequent polynomials can be generated by the recursion $$(n + 1) P_{n+1} (x) = (2n + 1) x P_n(x) - n P_{n - 1}(x).$$

For the case $$M=5$$, plot the data, the mean predictions corresponding to the MLE and MAP estimates for $$w$$, and $$100$$ samples from the predictive posterior distribution.
Which set of features works best for this function and why?

## Question 4 [20 Pts]:

Write a logistic regression model to classify the letters from the [EMNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset) letters dataset. Create an 80/20 train/test split of the data. You've been provided a [template](https://github.com/PredictiveIntelligenceLab/ENM5310/blob/main/assignments/assn2.ipynb) to carry this out. Fill in the suitable sections of the code.

- First train your model to perform binary classification between the letters b and d. Visualize performance on the training and test sets using a confusion matrix.
- Next, train your model to perform multiclass classification between b, d, p and q. Visualize performance on the training and test sets using a confusion matrix. 

## Question 5 [20 Pts]:
Write Python routines:
- `gradient_descent(f, g, x0, eta)` that performs an iteration of gradient descent on a given function $$f(x)$$ and its gradient $$g(x)$$, starting at a given point $$x_0$$, and using a given step-size $\eta$.
- `newton(f, g, H, x0, eta)` that performs an iteration of Newton's method on a given function $$f(x)$$, its gradient $$g(x)$$ and Hessian $$H(x)$$, starting at a given point $$x_0$$, and using a given step-size $$\eta$$.

Do **NOT** use `jax.grad` or `jax.hessian` (you need to define the gradient and Hessian functions). Use those routines in an optimization loop to optimize the following two functions:

- $$f_1(x, y) = x^2 + 100 y^2 $$
- $$f_2(x, y) = (1 - x)^2 + 100(y - x^2)^2,$$

Consider step-sizes $$\eta = 0.001, 0.01, 1$$. Take your initial starting location as $$x_0\in(-1, 1)$$. Comment on the following: 
- What is the effect of step-size on the different problems, and algorithms? 
- When the algorithm does converge, how many steps does it take to reach the minimum?

Give suitable reasons for your observations. Plot the objective function value vs the iterate number, and a plot that shows the progress on a contour plot of the landscape. Use these to support your reasoning.