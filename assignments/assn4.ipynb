{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "## Problem 1 [20 Points]:\n",
    "\n",
    "Deeplearning has great potential at making in-roads with traditional scientific and engineering research. In this question, we'll consider one good example of this where we'll be performing some predictions working with the [Higgs Dataset](http://mlphysics.ics.uci.edu/data/higgs/) which has been generated from Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. Note that this dataset consists of 11 million datapoints. We'll be using a subset of these for training as described below.\n",
    "\n",
    "Our classification task is that given the 28 features, we want to predict whether the observation is a “signal” (class 1) or the background (class 0). Perform an 80:20 split of the data into training and testing. Before passing your inputs to the model, normalize your inputs to have zero mean and unit variance.\n",
    "\n",
    "Build a model using ReLU for activation and glorot-normal initialization. Use binary cross entropy as the loss function, and perform the optimization using the Adam optimizer with default settings. Train your model till you observe a convergence of the loss curves. Plot curves showing evolution of the loss and accuracy when evaluated over the training and the testing data. Consider a 2 layer MLP of width 64.\n",
    "\n",
    "- Consider your dataset to be randomly selected 1100 data-points from the main dataset. Perform the train / test split on this subset and use these for training your model. What do you observe in this case?\n",
    "- Consider your dataset to be randomly selected 110000 data-points from the main dataset. Again, perform the train / test split on this subset and use these for training your model. Do your results differ from the earlier case? \n",
    "- One way to quantify overfitting is by understanding how sensitive the function is to input noise. For any function f(x), we have that the sensitivity can be computed as\n",
    "$$\\int \\left(\\frac{df}{dx}\\right)^2 p(x) dx = \\mathbb{E}((f')^2)$$\n",
    "This can be estimated by adding some small noise $w$ to the input $x$. For a small enough noise, we have that:\n",
    "$$f(x + w) \\approx f(x) + \\frac{df}{dx} w$$ \n",
    "We can choose the noise to be drawn from a normal distribution with mean 0 and variance $\\sigma^2(w)$. The variance of $f(x + w)$ is given by:\n",
    "$$\\sigma^2(f(x + w)) = \\sigma^2(f(x) + f'(x)w) = \\sigma^2(f) + \\sigma^2(w) \\mathbb{E}((f')^2)$$\n",
    "Using this strategy, estimate the sensitivity of the 2 models you trained earlier. Note that you need to estimate over several trials and take the average. Is it consistent with your findings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 2 [20 Points]:\n",
    "\n",
    "Train a convolutional neural network on the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). The network architecture should be as follows:\n",
    "\n",
    "- Input layer: Accepts 32x32x3 images\n",
    "- Convolution layer: 6 filters, filter size 5x5x3, stride 1\n",
    "- ReLU layer: Accepts and returns a 32x32x6 tensor\n",
    "- Max pooling layer: Pooling size 2x2, stride 2\n",
    "- Convolution layer: 12 filters, filter size 5x5x6, stride 1\n",
    "- ReLU layer: Accepts and returns a 16x16x12 tensor\n",
    "- Max pooling layer: Pooling size 2x2, stride 2\n",
    "- Convolution layer: 24 filters, filter size 5x5x12, stride 1\n",
    "- ReLU layer: Accepts and returns a 8x8x24 tensor\n",
    "- Max pooling layer: Pooling size 2x2, stride 2\n",
    "- Fully connected network with 2 hidden layers: Accepts a flattened tensor of dimension 4x4x24 and outputs a 10-dimensional tensor containing the predicted class labels. The layer dimensions should be [384,120,84,10].\n",
    "\n",
    "Notice how in each layer the number of filters is doubled, while the feature resolution is halved (_hint: make sure you use the appropriate amount of padding to achieve this_). Use the multi-class cross entropy loss function and the Adam optimizer with a learning rate of $10^{-3}$. Train the network for a total of 20000 iterations using a batch size of 128 images. Report the loss as a function of the training iterations, and the resulting confusion matrix for the test data-set. What do you notice here? Propose changes to improve the accuracy over the test set, and verify that you see an improvement of atleast 3% with your changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 3 [20 Points]:\n",
    "\n",
    "A subcritically damped pendulum is described by the equation\n",
    "$$x'' = -\\sin(x) - \\mu x'$$\n",
    "\n",
    "where $x$ is the angle of the pendulum and $\\mu$ is a damping parameter.\n",
    "\n",
    "(i) Create a time-series data-set by using [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html) to integrate this system in the interval $t\\in[0,50]$ using $2,000$ time-steps using an initial condition of $x(t=0) = 0$,  $x'(t=0) = 1.2$. Plot the evolution of the pendulum angle and angular velocity as a function of time.\n",
    "\n",
    "(ii) Implement an LSTM (Long short-term memory) with 8 lags, one hidden layer with 32 neurons, and a hyperbolic tangent activation function. Train the network using the data-set generated for $20,000$ stochastic gradient descent steps using the Adam optimizer with a learning rate of $10^{-3}$ and a mini-batch size of 128. Use the trained model to perform **one-step-ahead predictions** until the final time $T=60$, and compare them against the true data. Plot the predicted and exact signals, and report their discrepancy within the test interval in the relative $\\mathbb{L}_{2}$ norm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 4 [20 Points]:\n",
    "\n",
    "Simulation of complex physical systems described by nonlinear partial differential equations (PDEs) is central to engineering and physical science. We'll now train a neural network to solve a PDE given the boundary conditions. Till now, we have trained an MLP through data and penalize the network until it learns what we desire. However, in the case of PDEs, we can use knowledge of the known PDE to guide the training. Using the core idea of [PINNs](https://www.sciencedirect.com/science/article/pii/S0021999118307125), train a network to solve the Poisson equation \n",
    "$$u_{xx} + u_{yy} = -\\sin (\\pi x) \\sin(\\pi y)$$\n",
    "with the following BCs:\n",
    "$$u(0, y) = u(1, y) = u(x, 1) = u(x, 1) = 0$$\n",
    "\n",
    "Utilize 10000 collocation points in the domain to enforce the PDE and 100 data-points on each boundary to enforce boundary condition. Compare your solution against the analytic solution and report error in the relative $\\mathbb{L}_{2}$ norm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 5 [20 Points]:\n",
    "\n",
    "Submit a two-page summary of your proposed final project. Make sure you clearly articulate the following:\n",
    "- An overview of the proposed problem and why it is important.\n",
    "- What are the challenges to be addressed.\n",
    "- What is the current state-of-the-art and which are its potential limitations.\n",
    "- What is the formal mathematical definition of the problem, and what is the technical approach you will employ.\n",
    "- What is the potential impact upon the successful completion of the project.\n",
    "\n",
    "Any figures or references should be included within the 2-page limit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# please attach a separate PDF document for this question"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
